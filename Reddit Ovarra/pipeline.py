import os
import argparse
from typing import List
from models import RedditPost
from api_utils import (
    fetch_reddit_posts, discover_subreddits, fetch_and_attach_comments,
    classify_posts_relevance, generate_ovarra_replies,
    posts_to_json, posts_from_json
)

CHECKPOINTS = [
    ("scraped_posts.json", "Scraping posts", "scrape_posts"),
    ("relevant_posts.json", "Classifying relevance", "classify_relevance"),
    ("posts_with_comments.json", "Fetching comments", "fetch_comments"),
    ("final_posts.json", "Generating Ovarra replies", "generate_replies")
]

DEFAULT_SUBREDDITS = ["CamGirlProblems", "OnlyFansAdvice", "CreatorAdvice", "SexWorkersOnly", "LegalAdvice"]
DEFAULT_KEYWORDS = ["onlyfans leak", "content stolen", "dmca help", "nsfw leak", "privacy violation", "content removal"]


def scrape_posts(subreddits: List[str], keywords: List[str], post_limit: int, debug: bool) -> List[RedditPost]:
    posts = []
    for subreddit in subreddits:
        for keyword in keywords:
            if debug:
                print(f"Scraping r/{subreddit} for '{keyword}'...")
            raw_posts = fetch_reddit_posts(subreddit, keyword, post_limit)
            for post in raw_posts:
                data = post.get('data', {})
                posts.append(RedditPost(
                    url=data.get('url', ''),
                    relevance=False,
                    title=data.get('title', ''),
                    subtitle=data.get('selftext', ''),
                    comments=[]
                ))
    if debug:
        print(f"Scraped {len(posts)} posts.")
    return posts

def main():
    parser = argparse.ArgumentParser(description="Reddit Ovarra Pipeline with Checkpoints")
    parser.add_argument('--subreddits', nargs='+', help='List of subreddits to search')
    parser.add_argument('--discover', action='store_true', help='Discover related subreddits')
    parser.add_argument('--seed-subreddit', default='CamGirlProblems', help='Seed subreddit for discovery')
    parser.add_argument('--keywords', nargs='+', default=DEFAULT_KEYWORDS, help='Keywords to search for')
    parser.add_argument('--post-limit', type=int, default=10, help='Posts per keyword')
    parser.add_argument('--debug', action='store_true', help='Enable debug prints')
    parser.add_argument('--force', action='store_true', help='Ignore checkpoints and re-run all steps')
    args = parser.parse_args()

    debug = args.debug
    force = args.force

    # Step 1: Subreddit selection
    if args.discover:
        if debug:
            print(f"Discovering subreddits from seed: {args.seed_subreddit}...")
        subreddits = list(discover_subreddits(args.seed_subreddit, args.keywords, args.post_limit))
        if debug:
            print(f"Discovered subreddits: {subreddits}")
    else:
        subreddits = args.subreddits if args.subreddits else DEFAULT_SUBREDDITS
        if debug:
            print(f"Using subreddits: {subreddits}")

    # Pipeline with checkpoints
    data = None
    for idx, (ckpt_file, step_desc, step_key) in enumerate(CHECKPOINTS):
        if not force and os.path.exists(ckpt_file):
            if debug:
                print(f"[Checkpoint] Loading {step_desc} from {ckpt_file}")
            data = posts_from_json(ckpt_file)
            continue
        if debug:
            print(f"[Step] {step_desc}...")
        if step_key == "scrape_posts":
            data = scrape_posts(subreddits, args.keywords, args.post_limit, debug)
        elif step_key == "classify_relevance":
            data = classify_posts_relevance(data, debug=debug)
        elif step_key == "fetch_comments":
            data = fetch_and_attach_comments(data)
        elif step_key == "generate_replies":
            data = generate_ovarra_replies(data, debug=debug)
        posts_to_json(data, ckpt_file)
        if debug:
            print(f"[Checkpoint] Saved {step_desc} to {ckpt_file}")

    # Print summary
    print("\nPipeline complete! Final posts with Ovarra replies:")
    for i, post in enumerate(data, 1):
        print(f"\nPost #{i}")
        print(f"Title: {post.title}")
        print(f"URL: {post.url}")
        print(f"Ovarra Reply: {post.ovarra_reply}")

if __name__ == "__main__":
    main() 